{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f574cdf-6351-4a83-8a24-97376a66da3f",
   "metadata": {},
   "source": [
    "This class represents a dataset of training samples. It provides methods to gather samples from a specified folder, convert the data into tensors, and calculate R-squared scores for model predictions. It also provides functions to perform training and testing of models using the gathered samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "dcee0382-440e-4873-a921-ddda37a7a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import model_creator\n",
    "from training_sample import training_sample\n",
    "import locations\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import locations as l\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class training_dataset:\n",
    "    def __init__(self, dataset_folder):\n",
    "        # Initializes the training_dataset object by gathering samples from the specified dataset folder.        \n",
    "        self.samples = self.gather_samples(dataset_folder)\n",
    "\n",
    "\n",
    "    def gather_samples(self, dataset_folder):\n",
    "        # Gathers all training samples from the specified folder.\n",
    "        # Filters files in the folder and creates training_sample objects for each file.\n",
    "        all_items = os.listdir(dataset_folder)\n",
    "        files = [item for item in all_items if os.path.isfile(os.path.join(dataset_folder, item))]\n",
    "        samples = []\n",
    "        for i in files:\n",
    "            sample = training_sample(os.path.join(dataset_folder, i))\n",
    "            samples.append(sample)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def return_as_tensors_split(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T']):\n",
    "        # Converts the dataset into tensors and splits it into training and testing sets.\n",
    "        # Uses specified feature and target columns for the split.\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[feature_columns + target_columns]], ignore_index=True)\n",
    "\n",
    "        \n",
    "        df.columns = feature_columns + target_columns\n",
    "        \n",
    "        features = df[feature_columns]\n",
    "        targets = df[target_columns]\n",
    "    \n",
    "        x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "    \n",
    "        x_train = torch.from_numpy(x_train.to_numpy(dtype=np.float32))\n",
    "        x_test = torch.from_numpy(x_test.to_numpy(dtype=np.float32))\n",
    "        y_train = torch.from_numpy(y_train.to_numpy(dtype=np.float32))\n",
    "        y_test = torch.from_numpy(y_test.to_numpy(dtype=np.float32))\n",
    "    \n",
    "        return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "    def return_as_tensors(self, columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T']):\n",
    "        # Converts the entire dataset into tensors without splitting.\n",
    "        # Uses specified feature and target columns for the conversion.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[columns + target_columns]], ignore_index=True)\n",
    "\n",
    "        df.columns = columns + target_columns\n",
    "        features = df[columns]\n",
    "        targets = df[target_columns]\n",
    "        features = torch.from_numpy(features.to_numpy(dtype=np.float32))\n",
    "        targets = torch.from_numpy(targets.to_numpy(dtype=np.float32))\n",
    "        return [features, targets]\n",
    "\n",
    "\n",
    "    def get_total_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the total R-squared score for the model's predictions on the entire dataset.\n",
    "        model = model_creator.MLP.create_and_load(model, input_size=len(features), output_size=len(targets))\n",
    "        model.eval()\n",
    "        data = self.return_as_tensors(features, targets)\n",
    "        features = data[0]\n",
    "        targets = data[1]\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "            predictions = predictions.flatten().tolist()\n",
    "\n",
    "        pearson = pearsonr(predictions, targets.flatten().tolist())\n",
    "        r2_score = pearson[0] ** 2\n",
    "        return float(r2_score)\n",
    "\n",
    "    def get_median_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the median R-squared score for the model's predictions.\n",
    "        # Uses median values of predictions for each file.\n",
    "        \n",
    "        medians = []\n",
    "        data = []\n",
    "        \n",
    "\n",
    "        for sample in self.samples:\n",
    "            medians.append(sample.predict_median(model, features, len(targets)))\n",
    "            dataDummy = sample.data[targets] \n",
    "            dataDummy = dataDummy.iloc[:1].reset_index(drop=True)\n",
    "            dataDummy = dataDummy.astype(float).values.tolist()\n",
    "            dataDummy = dataDummy[0][0]\n",
    "            data.append(dataDummy)            \n",
    "\n",
    "        pearson = pearsonr(medians, data)\n",
    "        r2_score = pearson[0] ** 2\n",
    "        \n",
    "        \n",
    "        return float(r2_score)\n",
    "\n",
    "    def get_mean_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the median R-squared score for the model's predictions.\n",
    "        # Uses mean values of predictions for each file.\n",
    "        means = []\n",
    "        data = []\n",
    "        \n",
    "\n",
    "        for sample in self.samples:\n",
    "            means.append(sample.predict_mean(model, features, len(targets)))\n",
    "            dataDummy = sample.data[targets] \n",
    "            dataDummy = dataDummy.iloc[:1].reset_index(drop=True)\n",
    "            dataDummy = dataDummy.astype(float).values.tolist()\n",
    "            dataDummy = dataDummy[0][0]\n",
    "            data.append(dataDummy)            \n",
    "        print (means)\n",
    "        print (data)\n",
    "        pearson = pearsonr(means, data)\n",
    "        r2_score = pearson[0] ** 2\n",
    "\n",
    "\n",
    "        def train_flattened(self,model_name = \"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [256, 128, 64, 32], loss = nn.MSELoss(), save_folder = \"models\"):\n",
    "\n",
    "            data = self.flat_return_as_tensors(feature_columns, target_columns)\n",
    "    \n",
    "            if model_name == \"default\":\n",
    "                code_layers = \"\"\n",
    "                for layer in hidden_layers:\n",
    "                    code_layers += str(layer) + \"_\"\n",
    "                code_layers = code_layers[:-1]\n",
    "                model_name = \"model\" + str(target_columns) + \"_\" + str(code_layers) + \".pth\"\n",
    "    \n",
    "            model = model_creator.MLP(input_size=497, output_size=4, hidden_layers=hidden_layers)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "            save_path = os.path.join(save_folder, model_name)\n",
    "            os.makedirs(save_folder, exist_ok=True)\n",
    "            training_class.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], save_path=save_path, batch_size=0)\n",
    "\n",
    "\n",
    "    def flat_train_standardized(self,model_name = \"defaultStandarized\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [512, 256, 128, 64], loss = nn.MSELoss(), save_folder = \"models\"):\n",
    "        data = self.flat_return_as_standardized_tensors(feature_columns, target_columns)\n",
    "\n",
    "        if model_name == \"defaultStandarized\":\n",
    "            code_layers = \"\"\n",
    "            for layer in hidden_layers:\n",
    "                code_layers += str(layer) + \"_\"\n",
    "            code_layers = code_layers[:-1]\n",
    "            model_name = \"modelStanderd\" + str(target_columns) + \"_\" + str(code_layers) + \".pth\"\n",
    "\n",
    "        model = model_creator.MLP(input_size=497, output_size=1, hidden_layers=hidden_layers)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=0.001)\n",
    "        save_path = os.path.join(save_folder, model_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        training_class.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], save_path=save_path, batch_size=0)\n",
    "\n",
    "\n",
    "    def flat_return_as_standardized_tensors(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], scalerName = \"standardScaler\"):\n",
    "        targetDf = pd.DataFrame()\n",
    "        featureDf = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            features, targets = sample.return_as_flat_df(feature_columns, target_columns)\n",
    "            featureDf = pd.concat([featureDf, features], ignore_index=True)\n",
    "            targetDf = pd.concat([targetDf, targets], ignore_index=True)\n",
    "\n",
    "        scaler_path = l.locations.get_scalers_dir()\n",
    "\n",
    "        with open(os.path.join(scaler_path, scalerName), \"rb\") as f:\n",
    "            x_scaler = pickle.load(f)\n",
    "\n",
    "        \n",
    "\n",
    "        with open(os.path.join(scaler_path, scalerName), \"rb\") as f:\n",
    "            y_scaler = pickle.load(f)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x_scaled = x_scaler.fit_transform(featureDf)\n",
    "        y_scaled = y_scaler.fit_transform(targetDf)\n",
    "\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "        x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "        x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "        y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "        y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "        return [x_train, x_test, y_train, y_test]\n",
    "\n",
    "\n",
    "    def create_row_scaler(self, columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']):\n",
    "        # Creates a StandardScaler for the specified columns in the dataset.\n",
    "        # Saves the scaler to a file for later use.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[columns]], ignore_index=True)\n",
    "\n",
    "        x_scaler = StandardScaler()\n",
    "        x_scaled = x_scaler.fit_transform(df)\n",
    "\n",
    "        scalers_path = l.locations.get_scalers_dir()\n",
    "\n",
    "        full_path = os.path.join(scalers_path, self.generate_scaler_name(columns))\n",
    "\n",
    "        with open(full_path, \"wb\") as f:\n",
    "            pickle.dump(x_scaler, f)\n",
    "\n",
    "    def generate_scaler_name(columns, prefix=\"scaler\"):\n",
    "        \"\"\"\n",
    "        Generates a name for the scaler based on the names of the columns it scales.\n",
    "    \n",
    "        Args:\n",
    "            columns (list): List of column names to be scaled.\n",
    "            prefix (str): Optional prefix for the scaler name. Default is \"scaler\".\n",
    "    \n",
    "        Returns:\n",
    "            str: Generated scaler name.\n",
    "        \"\"\"\n",
    "        # Join column names with underscores and prepend the prefix\n",
    "        column_part = \"_\".join(columns)\n",
    "        scaler_name = f\"{prefix}_{column_part}.pkl\"\n",
    "        return scaler_name\n",
    "\n",
    "\n",
    "    def train(self, model_name = \"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [64, 32, 32, 16], loss = nn.MSELoss(), save_folder = \"models\"):\n",
    "\n",
    "        data = self.return_as_tensors_split(feature_columns, target_columns)\n",
    "        \n",
    "        model_name = self.generate_model_name(feature_columns, target_columns, False, hidden_layers)\n",
    "        \n",
    "        model = model_creator.MLP(len(features), len(targets), hidden_layers=hidden_layers)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        save_path = os.path.join(save_folder, model_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        training_class.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], save_path=save_path, batch_size=0)\n",
    "\n",
    "    def generate_model_name(feature_columns, target_columns, is_standardized, hidden_layers, prefix=\"model\"):\n",
    "        \"\"\"\n",
    "        Generates a model name based on features, targets, standardization, and hidden layers.\n",
    "    \n",
    "        Args:\n",
    "            feature_columns (list): List of feature column names.\n",
    "            target_columns (list): List of target column names.\n",
    "            is_standardized (bool): Whether the data is standardized.\n",
    "            hidden_layers (list): List of integers representing the number of neurons in each hidden layer.\n",
    "            prefix (str): Optional prefix for the model name. Default is \"model\".\n",
    "    \n",
    "        Returns:\n",
    "            str: Generated model name.\n",
    "        \"\"\"\n",
    "        # Join feature and target column names\n",
    "        features_part = \"_\".join(feature_columns)\n",
    "        targets_part = \"_\".join(target_columns)\n",
    "        \n",
    "        # Add standardization information\n",
    "        standardization_part = \"standardized\" if is_standardized else \"non_standardized\"\n",
    "        \n",
    "        # Add hidden layer information\n",
    "        hidden_layers_part = \"_\".join(map(str, hidden_layers))\n",
    "        \n",
    "        # Combine all parts into the model name\n",
    "        model_name = f\"{prefix}_{features_part}_to_{targets_part}_{standardization_part}_layers_{hidden_layers_part}.pth\"\n",
    "        return model_name\n",
    "\n",
    "\n",
    "    def flat_test_r2_standarized(self, x_Scaler_name, y_Scaler_name, model_name=\"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T']):\n",
    "        # Load the model\n",
    "        model = model_creator.MLP.create_and_load(model_name, input_size=len(feature_columns)*71, output_size=len(target_columns))\n",
    "        model.eval()\n",
    "    \n",
    "        # Load the data\n",
    "        data = self.return_as_flat_standardized_tensors(feature_columns, target_columns)\n",
    "        features_tr, features_te, targets_tr, targets_te = data\n",
    "    \n",
    "        # Perform predictions\n",
    "        with torch.no_grad():\n",
    "            pred_tr = model(features_tr)\n",
    "            pred_te = model(features_te)\n",
    "    \n",
    "        # Calculate R2 scores for each target column\n",
    "        train_r2_scores = {}\n",
    "        test_r2_scores = {}\n",
    "    \n",
    "        for i, target in enumerate(target_columns):\n",
    "            pred_tr_col = pred_tr[:, i].tolist()\n",
    "            pred_te_col = pred_te[:, i].tolist()\n",
    "            target_tr_col = targets_tr[:, i].tolist()\n",
    "            target_te_col = targets_te[:, i].tolist()\n",
    "    \n",
    "            train_r2_scores[target] = pearsonr(pred_tr_col, target_tr_col)[0] ** 2\n",
    "            test_r2_scores[target] = pearsonr(pred_te_col, target_te_col)[0] ** 2\n",
    "    \n",
    "        # Print R2 scores\n",
    "        for target in target_columns:\n",
    "            print(f\"{target} Train R2: {train_r2_scores[target]}\")\n",
    "            print(f\"{target} Test R2: {test_r2_scores[target]}\")\n",
    "\n",
    "    def df_split(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42):\n",
    "        # Splits the dataset into features and targets.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[feature_columns + target_columns]], ignore_index=True)\n",
    "\n",
    "        df.columns = feature_columns + target_columns\n",
    "        features = df[feature_columns]\n",
    "        targets = df[target_columns]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "        return [ x_train, x_test, y_train, y_test]\n",
    "\n",
    "    def flat_df_split(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42):\n",
    "        # Splits the dataset into features and targets.\n",
    "        dfFeatures = pd.DataFrame()\n",
    "        dfTargets = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            data = sample.return_as_flat_df(feature_columns, target_columns)\n",
    "            dfFeatures = pd.concat([dfFeatures, data[0]], ignore_index=True)\n",
    "            dfTargets = pd.concat([dfTargets, data[1]], ignore_index=True)\n",
    "\n",
    "        \n",
    "            \n",
    "        x_train, x_test, y_train, y_test = train_test_split(dfFeatures, dfTargets, test_size=0.2, random_state=42)\n",
    "\n",
    "        return [ x_train, x_test, y_train, y_test]\n",
    "\n",
    "\n",
    "    def poly_lin_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42,degree = 2):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize polynomial feature transformer and linear regression model\n",
    "        poly_transformer = PolynomialFeatures(degree=degree)\n",
    "        lin_reg = LinearRegression()\n",
    "        \n",
    "        # Transform features into polynomial features\n",
    "        polyXtrain = poly_transformer.fit_transform(data[0])\n",
    "        polyXtest = poly_transformer.transform(data[1])\n",
    "        \n",
    "        # Fit the linear regression model on the training data\n",
    "        lin_reg.fit(polyXtrain, data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lin_reg.predict(polyXtrain)\n",
    "        pred_test = lin_reg.predict(polyXtest)\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def flat_poly_lin_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42,degree = 2):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.flat_df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize polynomial feature transformer and linear regression model\n",
    "        poly_transformer = PolynomialFeatures(degree=degree)\n",
    "        lin_reg = LinearRegression()\n",
    "        \n",
    "        # Transform features into polynomial features\n",
    "        polyXtrain = poly_transformer.fit_transform(data[0])\n",
    "        polyXtest = poly_transformer.transform(data[1])\n",
    "        \n",
    "        # Fit the linear regression model on the training data\n",
    "        lin_reg.fit(polyXtrain, data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lin_reg.predict(polyXtrain)\n",
    "        pred_test = lin_reg.predict(polyXtest)\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def lin_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize linear regression model\n",
    "        lin_reg = LinearRegression()\n",
    "        \n",
    "        # Fit the linear regression model on the training data\n",
    "        lin_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lin_reg.predict(data[0])\n",
    "        pred_test = lin_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def flat_lin_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.flat_df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize linear regression model\n",
    "        lin_reg = LinearRegression()\n",
    "        \n",
    "        # Fit the linear regression model on the training data\n",
    "        lin_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lin_reg.predict(data[0])\n",
    "        pred_test = lin_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def lasso_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42, alpha = 0.1, ):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize Lasso regression model\n",
    "        lasso_reg = Lasso(alpha=alpha)\n",
    "        \n",
    "        # Fit the Lasso regression model on the training data\n",
    "        lasso_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lasso_reg.predict(data[0])\n",
    "        pred_test = lasso_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "\n",
    "    def flat_lasso_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42, alpha = 0.1):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.flat_df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize Lasso regression model\n",
    "        lasso_reg = Lasso(alpha=alpha)\n",
    "        \n",
    "        # Fit the Lasso regression model on the training data\n",
    "        lasso_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lasso_reg.predict(data[0])\n",
    "        pred_test = lasso_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "\n",
    "    def ridge_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42, alpha = 0.1):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize Ridge regression model\n",
    "        ridge_reg = Ridge(alpha=alpha)\n",
    "        \n",
    "        # Fit the Ridge regression model on the training data\n",
    "        ridge_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = ridge_reg.predict(data[0])\n",
    "        pred_test = ridge_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def flat_ridge_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42, alpha = 0.1):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.flat_df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize Ridge regression model\n",
    "        ridge_reg = Ridge(alpha=alpha)\n",
    "        \n",
    "        # Fit the Ridge regression model on the training data\n",
    "        ridge_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = ridge_reg.predict(data[0])\n",
    "        pred_test = ridge_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4cf51e9a-8519-46a6-933c-bf7fc062a16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'half_new_Si_jaw_delta',\n",
       " 'lifesat',\n",
       " 'new_Si_jaw_delta',\n",
       " 'quarter_new_Si_jaw_delta',\n",
       " 'R2_comparison_2.ipynb',\n",
       " 'si02',\n",
       " 'Si_jaw',\n",
       " 'Si_jaw_delta']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.locations.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1ec3030c-5bd5-4e18-b2af-17b6756b5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = l.locations.get_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c2edcaba-5359-4951-b622-593db16c3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_dataset_location = os.path.join(datasets_folder, \"new_Si_jaw_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a410570a-bc31-4dff-b7d8-af714d623786",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = training_dataset(specific_dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8e2e3b02-b339-48ac-8967-86227b70aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.05054489654328276\n",
      "Test R²: -0.05914663455717428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.05054489654328276, 'test_r2': -0.05914663455717428}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset.poly_lin_reg(target_columns=['B'], degree = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4dee8f2a-d5fb-4559-8d53-a66ffb4ee82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.9994664109538599\n",
      "Test R²: 0.8447401528555984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.9994664109538599, 'test_r2': 0.8447401528555984}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset.flat_poly_lin_reg(target_columns=['T'], degree = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e5418931-a6db-42a9-84fa-d0bae7af99a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.16417086758462096\n",
      "Test R²: 0.16697662361591215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.16417086758462096, 'test_r2': 0.16697662361591215}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset.lin_reg(target_columns=['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "988d02f5-7a95-4eff-aade-6a3550980630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.9346430749920622\n",
      "Test R²: -5.758235575319183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.9346430749920622, 'test_r2': -5.758235575319183}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset.flat_lin_reg(target_columns=['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2c23fcdd-9dca-4174-9e7f-2fc0c570580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.11607672360492705\n",
      "Test R²: 0.11674720597511923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.11607672360492705, 'test_r2': 0.11674720597511923}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset.lasso_reg(target_columns=['A'], alpha = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2787c80f-0ed1-4dcd-93a7-8c3278da0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.7082117537532235\n",
      "Test R²: 0.7091527408976392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_r2': 0.7082117537532235, 'test_r2': 0.7091527408976392}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset.flat_lasso_reg(target_columns=['A'], alpha = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2edcc6d-a518-4502-b5ff-86c1538ff694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

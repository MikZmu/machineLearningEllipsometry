{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f574cdf-6351-4a83-8a24-97376a66da3f",
   "metadata": {},
   "source": [
    "This class represents a dataset of training samples. It provides methods to gather samples from a specified folder, convert the data into tensors, and calculate R-squared scores for model predictions. It also provides functions to perform training and testing of models using the gathered samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcee0382-440e-4873-a921-ddda37a7a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import model_creator\n",
    "import model_training\n",
    "from training_sample import training_sample\n",
    "import locations\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import locations as l\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "class training_dataset:\n",
    "    def __init__(self, dataset_folder):\n",
    "        # Initializes the training_dataset object by gathering samples from the specified dataset folder.        \n",
    "        self.samples = self.gather_samples(dataset_folder)\n",
    "\n",
    "\n",
    "    def gather_samples(self, dataset_folder):\n",
    "        # Gathers all training samples from the specified folder.\n",
    "        # Filters files in the folder and creates training_sample objects for each file.\n",
    "        all_items = os.listdir(dataset_folder)\n",
    "        files = [item for item in all_items if os.path.isfile(os.path.join(dataset_folder, item))]\n",
    "        samples = []\n",
    "        for i in files:\n",
    "            sample = training_sample(os.path.join(dataset_folder, i))\n",
    "            samples.append(sample)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def return_as_tensors_split(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T']):\n",
    "        # Converts the dataset into tensors and splits it into training and testing sets.\n",
    "        # Uses specified feature and target columns for the split.\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[feature_columns + target_columns]], ignore_index=True)\n",
    "\n",
    "        \n",
    "        df.columns = feature_columns + target_columns\n",
    "        \n",
    "        features = df[feature_columns]\n",
    "        targets = df[target_columns]\n",
    "    \n",
    "        x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "    \n",
    "        x_train = torch.from_numpy(x_train.to_numpy(dtype=np.float32))\n",
    "        x_test = torch.from_numpy(x_test.to_numpy(dtype=np.float32))\n",
    "        y_train = torch.from_numpy(y_train.to_numpy(dtype=np.float32))\n",
    "        y_test = torch.from_numpy(y_test.to_numpy(dtype=np.float32))\n",
    "    \n",
    "        return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "    def return_as_tensors(self, columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T']):\n",
    "        # Converts the entire dataset into tensors without splitting.\n",
    "        # Uses specified feature and target columns for the conversion.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[columns + target_columns]], ignore_index=True)\n",
    "\n",
    "        df.columns = columns + target_columns\n",
    "        features = df[columns]\n",
    "        targets = df[target_columns]\n",
    "        features = torch.from_numpy(features.to_numpy(dtype=np.float32))\n",
    "        targets = torch.from_numpy(targets.to_numpy(dtype=np.float32))\n",
    "        return [features, targets]\n",
    "\n",
    "\n",
    "    def get_total_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the total R-squared score for the model's predictions on the entire dataset.\n",
    "        model = model_creator.MLP.create_and_load(model, input_size=len(features), output_size=len(targets))\n",
    "        model.eval()\n",
    "        data = self.return_as_tensors(features, targets)\n",
    "        features = data[0]\n",
    "        targets = data[1]\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "            predictions = predictions.flatten().tolist()\n",
    "\n",
    "        pearson = pearsonr(predictions, targets.flatten().tolist())\n",
    "        r2_score = pearson[0] ** 2\n",
    "        return float(r2_score)\n",
    "\n",
    "    def get_median_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the median R-squared score for the model's predictions.\n",
    "        # Uses median values of predictions for each file.\n",
    "        \n",
    "        medians = []\n",
    "        data = []\n",
    "        \n",
    "\n",
    "        for sample in self.samples:\n",
    "            medians.append(sample.predict_median(model, features, len(targets)))\n",
    "            dataDummy = sample.data[targets] \n",
    "            dataDummy = dataDummy.iloc[:1].reset_index(drop=True)\n",
    "            dataDummy = dataDummy.astype(float).values.tolist()\n",
    "            dataDummy = dataDummy[0][0]\n",
    "            data.append(dataDummy)            \n",
    "\n",
    "        pearson = pearsonr(medians, data)\n",
    "        r2_score = pearson[0] ** 2\n",
    "        \n",
    "        \n",
    "        return float(r2_score)\n",
    "\n",
    "    def get_mean_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the median R-squared score for the model's predictions.\n",
    "        # Uses mean values of predictions for each file.\n",
    "        means = []\n",
    "        data = []\n",
    "        \n",
    "\n",
    "        for sample in self.samples:\n",
    "            means.append(sample.predict_mean(model, features, len(targets)))\n",
    "            dataDummy = sample.data[targets] \n",
    "            dataDummy = dataDummy.iloc[:1].reset_index(drop=True)\n",
    "            dataDummy = dataDummy.astype(float).values.tolist()\n",
    "            dataDummy = dataDummy[0][0]\n",
    "            data.append(dataDummy)            \n",
    "        print (means)\n",
    "        print (data)\n",
    "        pearson = pearsonr(means, data)\n",
    "        r2_score = pearson[0] ** 2\n",
    "\n",
    "\n",
    "        def train_flattened(self,model_name = \"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [256, 128, 64, 32], loss = nn.MSELoss(), save_folder = \"models\"):\n",
    "\n",
    "            data = self.flat_return_as_tensors(feature_columns, target_columns)\n",
    "    \n",
    "            if model_name == \"default\":\n",
    "                code_layers = \"\"\n",
    "                for layer in hidden_layers:\n",
    "                    code_layers += str(layer) + \"_\"\n",
    "                code_layers = code_layers[:-1]\n",
    "                model_name = \"model\" + str(target_columns) + \"_\" + str(code_layers) + \".pth\"\n",
    "    \n",
    "            model = model_creator.MLP(input_size=497, output_size=4, hidden_layers=hidden_layers)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "            save_path = os.path.join(save_folder, model_name)\n",
    "            model_training.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], save_path=save_path, batch_size=0)\n",
    "\n",
    "\n",
    "    def flat_train_standardized(self,model_name = \"defaultStandarized\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [128, 64, 32], loss = nn.MSELoss, bs = 0):\n",
    "        data = self.flat_return_as_standardized_tensors(feature_columns, target_columns,\n",
    "        'scaler_wavelength_psi65_del65_psi70_del70_psi75_del75.pkl',\n",
    "        \"scaler_T_A_B_C.pkl\")\n",
    "\n",
    "        if model_name == \"defaultStandarized\":\n",
    "            model_name = self.generate_model_name(feature_columns, target_columns, True, hidden_layers)\n",
    "\n",
    "        model = model_creator.MLP(input_size=len(feature_columns)*71, output_size=len(target_columns), hidden_layers=hidden_layers)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=0.001)\n",
    "        model_training.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], model_name, batch_size=bs)\n",
    "\n",
    "\n",
    "    def flat_return_as_standardized_tensors(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'],\n",
    "                                            target_columns = ['T'],\n",
    "                                            XscalerName = \"ss\",\n",
    "                                            YscalerName = \"pp\"):\n",
    "        targetDf = pd.DataFrame()\n",
    "        featureDf = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            features, targets = sample.return_as_flat_df(feature_columns, target_columns)\n",
    "            featureDf = pd.concat([featureDf, features], ignore_index=True)\n",
    "            targetDf = pd.concat([targetDf, targets], ignore_index=True)\n",
    "\n",
    "        scaler_path = l.locations.get_scalers_dir()\n",
    "\n",
    "        with open(os.path.join(scaler_path, XscalerName), \"rb\") as f:\n",
    "            x_scaler = pickle.load(f)\n",
    "\n",
    "        \n",
    "\n",
    "        with open(os.path.join(scaler_path, YscalerName), \"rb\") as f:\n",
    "            y_scaler = pickle.load(f)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x_scaled = x_scaler.fit_transform(featureDf)\n",
    "        y_scaled = y_scaler.fit_transform(targetDf)\n",
    "\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "        x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "        x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "        y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "        y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "        return [x_train, x_test, y_train, y_test]\n",
    "\n",
    "\n",
    "    def create_row_scaler(self, columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']):\n",
    "        # Creates a StandardScaler for the specified columns in the dataset.\n",
    "        # Saves the scaler to a file for later use.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[columns]], ignore_index=True)\n",
    "\n",
    "        x_scaler = StandardScaler()\n",
    "        x_scaled = x_scaler.fit_transform(df)\n",
    "\n",
    "        scalers_path = l.locations.get_scalers_dir()\n",
    "\n",
    "        full_path = os.path.join(scalers_path, self.generate_scaler_name(columns))\n",
    "\n",
    "        with open(full_path, \"wb\") as f:\n",
    "            pickle.dump(x_scaler, f)\n",
    "\n",
    "    def flat_create_scaler(self, col=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']):\n",
    "        # Creates a StandardScaler for the specified columns in the dataset.\n",
    "        # Saves the scaler to a file for later use.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            features, targets = sample.return_as_flat_df(col, ['T'])\n",
    "            df = pd.concat([df, features], ignore_index=True)\n",
    "\n",
    "        x_scaler = StandardScaler()\n",
    "        x_scaled = x_scaler.fit_transform(df)\n",
    "        scalers_path = l.locations.get_scalers_dir()\n",
    "        print(col)\n",
    "        trol = col\n",
    "        scalerName = self.generate_scaler_name(trol)\n",
    "\n",
    "        full_path = os.path.join(scalers_path, scalerName)\n",
    "\n",
    "        with open(full_path, \"wb\") as f:\n",
    "            pickle.dump(x_scaler, f)\n",
    "\n",
    "\n",
    "    def generate_scaler_name(self, columns, prefix=\"scaler\"):\n",
    "            \"\"\"\n",
    "            Generates a name for the scaler based on the names of the columns it scales.\n",
    "        \n",
    "            Args:\n",
    "                columns (list): List of column names to be scaled.\n",
    "                prefix (str): Optional prefix for the scaler name. Default is \"scaler\".\n",
    "        \n",
    "            Returns:\n",
    "                str: Generated scaler name.\n",
    "            \"\"\"\n",
    "            # Ensure columns is an iterable\n",
    "            print(columns)\n",
    "\n",
    "            if not isinstance(columns, (list, tuple)):\n",
    "                raise TypeError(\"columns must be a list or tuple of column names.\")\n",
    "            \n",
    "            # Join column names with underscores and prepend the prefix\n",
    "            column_part = \"_\".join(columns)\n",
    "            scaler_name = f\"{prefix}_{column_part}.pkl\"\n",
    "            return scaler_name\n",
    "\n",
    "\n",
    "    def train(self, model_name = \"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [64, 32, 32, 16], loss = nn.MSELoss(), save_folder = \"models\"):\n",
    "\n",
    "        data = self.return_as_tensors_split(feature_columns, target_columns)\n",
    "        \n",
    "        model_name = self.generate_model_name(feature_columns, target_columns, False, hidden_layers)\n",
    "        \n",
    "        model = model_creator.MLP(len(features), len(targets), hidden_layers=hidden_layers)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        save_path = os.path.join(save_folder, model_name)\n",
    "        model_training.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], save_path=save_path, batch_size=0)\n",
    "\n",
    "    def generate_model_name(self,feature_columns, target_columns, is_standardized, hidden_layers, prefix=\"model\"):\n",
    "        \"\"\"\n",
    "        Generates a model name based on features, targets, standardization, and hidden layers.\n",
    "    \n",
    "        Args:\n",
    "            feature_columns (list): List of feature column names.\n",
    "            target_columns (list): List of target column names.\n",
    "            is_standardized (bool): Whether the data is standardized.\n",
    "            hidden_layers (list): List of integers representing the number of neurons in each hidden layer.\n",
    "            prefix (str): Optional prefix for the model name. Default is \"model\".\n",
    "    \n",
    "        Returns:\n",
    "            str: Generated model name.\n",
    "        \"\"\"\n",
    "        # Join feature and target column names\n",
    "        features_part = \"_\".join(feature_columns)\n",
    "        targets_part = \"_\".join(target_columns)\n",
    "        \n",
    "        # Add standardization information\n",
    "        standardization_part = \"standardized\" if is_standardized else \"non_standardized\"\n",
    "        \n",
    "        # Add hidden layer information\n",
    "        hidden_layers_part = \"_\".join(map(str, hidden_layers))\n",
    "        \n",
    "        # Combine all parts into the model name\n",
    "        model_name = f\"{prefix}_{features_part}_to_{targets_part}_{standardization_part}_layers_{hidden_layers_part}.pth\"\n",
    "        return model_name\n",
    "\n",
    "\n",
    "    def flat_test_r2_standarized(self, x_Scaler_name, y_Scaler_name, model_name=\"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T']):\n",
    "        # Load the model\n",
    "        model = model_creator.MLP.create_and_load(model_name, input_size=len(feature_columns)*71, output_size=len(target_columns))\n",
    "        model.eval()\n",
    "    \n",
    "        # Load the data\n",
    "        data = self.flat_return_as_standardized_tensors(feature_columns, target_columns,x_Scaler_name,y_Scaler_name)\n",
    "        features_tr, features_te, targets_tr, targets_te = data\n",
    "    \n",
    "        # Perform predictions\n",
    "        with torch.no_grad():\n",
    "            pred_tr = model(features_tr)\n",
    "            pred_te = model(features_te)\n",
    "    \n",
    "        # Calculate R2 scores for each target column\n",
    "        train_r2_scores = {}\n",
    "        test_r2_scores = {}\n",
    "    \n",
    "        for i, target in enumerate(target_columns):\n",
    "            pred_tr_col = pred_tr[:, i].tolist()\n",
    "            pred_te_col = pred_te[:, i].tolist()\n",
    "            target_tr_col = targets_tr[:, i].tolist()\n",
    "            target_te_col = targets_te[:, i].tolist()\n",
    "    \n",
    "            train_r2_scores[target] = pearsonr(pred_tr_col, target_tr_col)[0] ** 2\n",
    "            test_r2_scores[target] = pearsonr(pred_te_col, target_te_col)[0] ** 2\n",
    "    \n",
    "        # Print R2 scores\n",
    "        for target in target_columns:\n",
    "            print(f\"{target} Train R2: {train_r2_scores[target]}\")\n",
    "            print(f\"{target} Test R2: {test_r2_scores[target]}\")\n",
    "\n",
    "    def df_split(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42):\n",
    "        # Splits the dataset into features and targets.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[feature_columns + target_columns]], ignore_index=True)\n",
    "\n",
    "        df.columns = feature_columns + target_columns\n",
    "        features = df[feature_columns]\n",
    "        targets = df[target_columns]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "        return [ x_train, x_test, y_train, y_test]\n",
    "\n",
    "    def flat_df_split(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42):\n",
    "        # Splits the dataset into features and targets.\n",
    "        dfFeatures = pd.DataFrame()\n",
    "        dfTargets = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            data = sample.return_as_flat_df(feature_columns, target_columns)\n",
    "            dfFeatures = pd.concat([dfFeatures, data[0]], ignore_index=True)\n",
    "            dfTargets = pd.concat([dfTargets, data[1]], ignore_index=True)\n",
    "\n",
    "        \n",
    "            \n",
    "        x_train, x_test, y_train, y_test = train_test_split(dfFeatures, dfTargets, test_size=0.2, random_state=42)\n",
    "\n",
    "        return [ x_train, x_test, y_train, y_test]\n",
    "\n",
    "\n",
    "    def poly_lin_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42,degree = 2):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize polynomial feature transformer and linear regression model\n",
    "        poly_transformer = PolynomialFeatures(degree=degree)\n",
    "        lin_reg = LinearRegression()\n",
    "        \n",
    "        # Transform features into polynomial features\n",
    "        polyXtrain = poly_transformer.fit_transform(data[0])\n",
    "        polyXtest = poly_transformer.transform(data[1])\n",
    "        \n",
    "        # Fit the linear regression model on the training data\n",
    "        lin_reg.fit(polyXtrain, data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lin_reg.predict(polyXtrain)\n",
    "        pred_test = lin_reg.predict(polyXtest)\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def flat_poly_lin_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42,degree = 2):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.flat_df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize polynomial feature transformer and linear regression model\n",
    "        poly_transformer = PolynomialFeatures(degree=degree)\n",
    "        lin_reg = LinearRegression()\n",
    "        \n",
    "        # Transform features into polynomial features\n",
    "        polyXtrain = poly_transformer.fit_transform(data[0])\n",
    "        polyXtest = poly_transformer.transform(data[1])\n",
    "        \n",
    "        # Fit the linear regression model on the training data\n",
    "        lin_reg.fit(polyXtrain, data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lin_reg.predict(polyXtrain)\n",
    "        pred_test = lin_reg.predict(polyXtest)\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def decode_model_name(model_name):\n",
    "        \"\"\"\n",
    "        Decodes a model name to extract the number of input neurons, output neurons, \n",
    "        hidden layer neurons, names of output variables, and input variables.\n",
    "    \n",
    "        Args:\n",
    "            model_name (str): The model name to decode.\n",
    "    \n",
    "        Returns:\n",
    "            dict: A dictionary containing the decoded information.\n",
    "        \"\"\"\n",
    "        # Remove the file extension\n",
    "        model_name = model_name.replace(\".pth\", \"\")\n",
    "        \n",
    "        # Split the model name into parts\n",
    "        parts = model_name.split(\"_\")\n",
    "        \n",
    "        # Extract input and output variable names\n",
    "        input_start = parts.index(\"model\") + 1\n",
    "        to_index = parts.index(\"to\")\n",
    "        feature_columns = parts[input_start:to_index]\n",
    "        \n",
    "        output_start = to_index + 1\n",
    "        standardization_index = parts.index(\"standardized\") if \"standardized\" in parts else parts.index(\"non_standardized\")\n",
    "        target_columns = parts[output_start:standardization_index]\n",
    "        \n",
    "        # Extract hidden layers\n",
    "        layers_index = parts.index(\"layers\") + 1\n",
    "        hidden_layers = list(map(int, parts[layers_index:]))\n",
    "        \n",
    "        # Calculate the number of input and output neurons\n",
    "        input_neurons = len(feature_columns)\n",
    "        output_neurons = len(target_columns)\n",
    "    \n",
    "        # Return the decoded information\n",
    "        return {\n",
    "            \"input_neurons\": input_neurons,\n",
    "            \"output_neurons\": output_neurons,\n",
    "            \"hidden_layers\": hidden_layers,\n",
    "            \"input_variables\": feature_columns,\n",
    "            \"output_variables\": target_columns\n",
    "        }\n",
    "\n",
    "\n",
    "    def lin_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize linear regression model\n",
    "        lin_reg = LinearRegression()\n",
    "        \n",
    "        # Fit the linear regression model on the training data\n",
    "        lin_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lin_reg.predict(data[0])\n",
    "        pred_test = lin_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def flat_lin_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.flat_df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize linear regression model\n",
    "        lin_reg = LinearRegression()\n",
    "        \n",
    "        # Fit the linear regression model on the training data\n",
    "        lin_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lin_reg.predict(data[0])\n",
    "        pred_test = lin_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def lasso_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42, alpha = 0.1, ):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize Lasso regression model\n",
    "        lasso_reg = Lasso(alpha=alpha)\n",
    "        \n",
    "        # Fit the Lasso regression model on the training data\n",
    "        lasso_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lasso_reg.predict(data[0])\n",
    "        pred_test = lasso_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "\n",
    "    def flat_lasso_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42, alpha = 0.1):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.flat_df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize Lasso regression model\n",
    "        lasso_reg = Lasso(alpha=alpha)\n",
    "        \n",
    "        # Fit the Lasso regression model on the training data\n",
    "        lasso_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = lasso_reg.predict(data[0])\n",
    "        pred_test = lasso_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "\n",
    "    def ridge_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42, alpha = 0.1):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize Ridge regression model\n",
    "        ridge_reg = Ridge(alpha=alpha)\n",
    "        \n",
    "        # Fit the Ridge regression model on the training data\n",
    "        ridge_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = ridge_reg.predict(data[0])\n",
    "        pred_test = ridge_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "    def flat_ridge_reg(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns=['T'], test_size=0.2, random_state = 42, alpha = 0.1):\n",
    "        # Split the dataset into training and testing sets\n",
    "        data = self.flat_df_split(feature_columns, target_columns, test_size, random_state)\n",
    "        \n",
    "        # Initialize Ridge regression model\n",
    "        ridge_reg = Ridge(alpha=alpha)\n",
    "        \n",
    "        # Fit the Ridge regression model on the training data\n",
    "        ridge_reg.fit(data[0], data[2])\n",
    "        \n",
    "        # Predict on train and test sets\n",
    "        pred_train = ridge_reg.predict(data[0])\n",
    "        pred_test = ridge_reg.predict(data[1])\n",
    "        \n",
    "        # Calculate R² coefficients\n",
    "        r2_train = r2_score(data[2], pred_train)\n",
    "        r2_test = r2_score(data[3], pred_test)\n",
    "        \n",
    "        # Print R² coefficients\n",
    "        print(f\"Train R²: {r2_train}\")\n",
    "        print(f\"Test R²: {r2_test}\")\n",
    "        \n",
    "        # Return R² coefficients as a dictionary\n",
    "        return {\"train_r2\": r2_train, \"test_r2\": r2_test}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

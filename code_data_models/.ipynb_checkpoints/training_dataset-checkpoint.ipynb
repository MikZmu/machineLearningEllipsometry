{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f574cdf-6351-4a83-8a24-97376a66da3f",
   "metadata": {},
   "source": [
    "This class represents a dataset of training samples. It provides methods to gather samples from a specified folder, convert the data into tensors, and calculate R-squared scores for model predictions. It also provides functions to perform training and testing of models using the gathered samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcee0382-440e-4873-a921-ddda37a7a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Train R2:  0.9993265501816238\n",
      "A Train R2:  0.9751036437304422\n",
      "B Train R2:  0.9660701704860761\n",
      "C Train R2:  0.4921649139042205\n",
      "T Test R2:  0.9992700699598391\n",
      "A Test R2:  0.9550196826709711\n",
      "B Test R2:  0.9463011802250868\n",
      "C Test R2:  0.0011005490648434936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfolder = os.getcwd()\\nparent_folder = os.path.dirname(folder)\\nfolder_path = os.path.join(parent_folder,\"code_data_models\",\"datasets\", \"new_Si_jaw_delta\", \"\")\\nmodels_dir = locations.models_dir\\ndataset = training_dataset(folder_path)\\nmodeldir = os.path.join(models_dir, \"modelA_64_32_32_16.pth\")\\nprint(modeldir)\\nprint(dataset.get_total_r2_score(modeldir, [\\'wavelength\\', \\'psi65\\', \\'del65\\', \\'psi70\\', \\'del70\\', \\'psi75\\', \\'del75\\'], [\\'A\\']))\\nprint(dataset.get_median_r2_score(modeldir, [\\'wavelength\\', \\'psi65\\', \\'del65\\', \\'psi70\\', \\'del70\\', \\'psi75\\', \\'del75\\'], [\\'A\\']))\\nprint(dataset.get_mean_r2_score(modeldir, [\\'wavelength\\', \\'psi65\\', \\'del65\\', \\'psi70\\', \\'del70\\', \\'psi75\\', \\'del75\\'], [\\'A\\']))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import os\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import model_creator\n",
    "from training_sample import training_sample\n",
    "import locations\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class training_dataset:\n",
    "    def __init__(self, dataset_folder):\n",
    "        # Initializes the training_dataset object by gathering samples from the specified dataset folder.        \n",
    "        self.samples = self.gather_samples(dataset_folder)\n",
    "\n",
    "\n",
    "    def gather_samples(self, dataset_folder):\n",
    "        # Gathers all training samples from the specified folder.\n",
    "        # Filters files in the folder and creates training_sample objects for each file.\n",
    "        all_items = os.listdir(dataset_folder)\n",
    "        files = [item for item in all_items if os.path.isfile(os.path.join(dataset_folder, item))]\n",
    "        samples = []\n",
    "        for i in files:\n",
    "            sample = training_sample(os.path.join(dataset_folder, i))\n",
    "            samples.append(sample)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def return_as_tensors_split(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T']):\n",
    "        # Converts the dataset into tensors and splits it into training and testing sets.\n",
    "        # Uses specified feature and target columns for the split.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            df.concat(sample.data[feature_columns], ignore_index=True)\n",
    "\n",
    "        features = df[feature_columns]\n",
    "        targets = df[target_columns]\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "        x_train = torch.from_numpy(x_train.to_numpy(dtype=np.float32))\n",
    "        x_test = torch.from_numpy(x_test.to_numpy(dtype=np.float32))\n",
    "        y_train = torch.from_numpy(y_train.to_numpy(dtype=np.float32))\n",
    "        y_test = torch.from_numpy(y_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "        return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "    def return_as_tensors(self, columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T']):\n",
    "        # Converts the entire dataset into tensors without splitting.\n",
    "        # Uses specified feature and target columns for the conversion.\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            df = pd.concat([df, sample.data[columns + target_columns]], ignore_index=True)\n",
    "\n",
    "        df.columns = columns + target_columns\n",
    "        features = df[columns]\n",
    "        targets = df[target_columns]\n",
    "        features = torch.from_numpy(features.to_numpy(dtype=np.float32))\n",
    "        targets = torch.from_numpy(targets.to_numpy(dtype=np.float32))\n",
    "        return [features, targets]\n",
    "\n",
    "\n",
    "    def get_total_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the total R-squared score for the model's predictions on the entire dataset.\n",
    "        model = model_creator.MLP.create_and_load(model, input_size=len(features), output_size=len(targets))\n",
    "        model.eval()\n",
    "        data = self.return_as_tensors(features, targets)\n",
    "        features = data[0]\n",
    "        targets = data[1]\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "            predictions = predictions.flatten().tolist()\n",
    "\n",
    "        pearson = pearsonr(predictions, targets.flatten().tolist())\n",
    "        r2_score = pearson[0] ** 2\n",
    "        return float(r2_score)\n",
    "\n",
    "    def get_median_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the median R-squared score for the model's predictions.\n",
    "        # Uses median values of predictions for each file.\n",
    "        \n",
    "        medians = []\n",
    "        data = []\n",
    "        \n",
    "\n",
    "        for sample in self.samples:\n",
    "            medians.append(sample.predict_median(model, features, len(targets)))\n",
    "            dataDummy = sample.data[targets] \n",
    "            dataDummy = dataDummy.iloc[:1].reset_index(drop=True)\n",
    "            dataDummy = dataDummy.astype(float).values.tolist()\n",
    "            dataDummy = dataDummy[0][0]\n",
    "            data.append(dataDummy)            \n",
    "\n",
    "        pearson = pearsonr(medians, data)\n",
    "        r2_score = pearson[0] ** 2\n",
    "        \n",
    "        \n",
    "        return float(r2_score)\n",
    "\n",
    "    def get_mean_r2_score(self, model, features = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], targets = ['T']):\n",
    "        # Calculates the median R-squared score for the model's predictions.\n",
    "        # Uses mean values of predictions for each file.\n",
    "        means = []\n",
    "        data = []\n",
    "        \n",
    "\n",
    "        for sample in self.samples:\n",
    "            means.append(sample.predict_mean(model, features, len(targets)))\n",
    "            dataDummy = sample.data[targets] \n",
    "            dataDummy = dataDummy.iloc[:1].reset_index(drop=True)\n",
    "            dataDummy = dataDummy.astype(float).values.tolist()\n",
    "            dataDummy = dataDummy[0][0]\n",
    "            data.append(dataDummy)            \n",
    "        print (means)\n",
    "        print (data)\n",
    "        pearson = pearsonr(means, data)\n",
    "        r2_score = pearson[0] ** 2\n",
    "\n",
    "\n",
    "        def train_flattened(self,model_name = \"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [256, 128, 64, 32], loss = nn.MSELoss(), save_folder = \"models\"):\n",
    "\n",
    "            data = self.return_as_flat_tensors(feature_columns, target_columns)\n",
    "    \n",
    "            if model_name == \"default\":\n",
    "                code_layers = \"\"\n",
    "                for layer in hidden_layers:\n",
    "                    code_layers += str(layer) + \"_\"\n",
    "                code_layers = code_layers[:-1]\n",
    "                model_name = \"model\" + str(target_columns) + \"_\" + str(code_layers) + \".pth\"\n",
    "    \n",
    "            model = model_creator.MLP(input_size=497, output_size=4, hidden_layers=hidden_layers)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "            save_path = os.path.join(save_folder, model_name)\n",
    "            os.makedirs(save_folder, exist_ok=True)\n",
    "            training_class.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], save_path=save_path, batch_size=0)\n",
    "\n",
    "\n",
    "    def train_standardized_flattened(self,model_name = \"defaultStandarized\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [512, 256, 128, 64], loss = nn.MSELoss(), save_folder = \"models\"):\n",
    "        data = self.return_as_flat_standardized_tensors(feature_columns, target_columns)\n",
    "\n",
    "        if model_name == \"defaultStandarized\":\n",
    "            code_layers = \"\"\n",
    "            for layer in hidden_layers:\n",
    "                code_layers += str(layer) + \"_\"\n",
    "            code_layers = code_layers[:-1]\n",
    "            model_name = \"modelStanderd\" + str(target_columns) + \"_\" + str(code_layers) + \".pth\"\n",
    "\n",
    "        model = model_creator.MLP(input_size=497, output_size=1, hidden_layers=hidden_layers)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=0.001)\n",
    "        save_path = os.path.join(save_folder, model_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        training_class.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], save_path=save_path, batch_size=0)\n",
    "\n",
    "\n",
    "    def return_as_flat_standardized_tensors(self, feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], scalerName = \"standardScaler\"):\n",
    "        targetDf = pd.DataFrame()\n",
    "        featureDf = pd.DataFrame()\n",
    "\n",
    "        for sample in self.samples:\n",
    "            features, targets = sample.return_as_flat_df(feature_columns, target_columns)\n",
    "            featureDf = pd.concat([featureDf, features], ignore_index=True)\n",
    "            targetDf = pd.concat([targetDf, targets], ignore_index=True)\n",
    "\n",
    "        x_scaler = StandardScaler()\n",
    "        y_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "        x_scaled = x_scaler.fit_transform(featureDf)\n",
    "        y_scaled = y_scaler.fit_transform(targetDf)\n",
    "\n",
    "\n",
    "        with open(scalerName + \"Y\" + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(y_scaler, f)\n",
    "\n",
    "        with open(scalerName + \"X\" + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(x_scaler, f)\n",
    "\n",
    "\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "        x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "        x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "        y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "        y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "        return [x_train, x_test, y_train, y_test]\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, model_name = \"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T'], hidden_layers = [64, 32, 32, 16], loss = nn.MSELoss(), save_folder = \"models\"):\n",
    "\n",
    "        data = self.return_as_tensors_split(feature_columns, target_columns)\n",
    "\n",
    "        if model_name == \"default\":\n",
    "            code_layers = \"\"\n",
    "            for layer in hidden_layers:\n",
    "                code_layers += str(layer) + \"_\"\n",
    "            code_layers = code_layers[:-1]\n",
    "            model_name = \"model\" + str(target_columns) + \"_\" + str(code_layers) + \".pth\"\n",
    "\n",
    "        model = model_creator.MLP(input_size=497, output_size=4, hidden_layers=hidden_layers)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        save_path = os.path.join(save_folder, model_name)\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        training_class.train_model(model, loss, optimizer, data[0], data[2], data[1], data[3], save_path=save_path, batch_size=0)\n",
    "\n",
    "\n",
    "    def test_r2_flattened(self, x_Scaler_name, y_Scaler_name, model_name = \"default\", feature_columns=['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75'], target_columns = ['T', 'A', 'B', 'C']):\n",
    "\n",
    "        model = model_creator.MLP.create_and_load(model_name, input_size=497, output_size=4)\n",
    "        model.eval()\n",
    "        # with open(x_Scaler_name + \".pkl\", \"rb\") as f:\n",
    "        #     x_scaler = pickle.load(f)\n",
    "        #\n",
    "        # with open(y_Scaler_name + \".pkl\", \"rb\") as f:\n",
    "        #     y_scaler = pickle.load(f)\n",
    "\n",
    "        data = self.return_as_flat_standardized_tensors(feature_columns, target_columns)\n",
    "        features_tr = data[0]\n",
    "        features_te = data[1]\n",
    "        targets_tr = data[2]\n",
    "        targets_te = data[3]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_tr = model(features_tr)\n",
    "            pred_te = model(features_te)\n",
    "\n",
    "        pred_tr = [pred_tr[:,i] for i in range(pred_tr.size(1))]\n",
    "        pred_te = [pred_te[:,i] for i in range(pred_te.size(1))]\n",
    "        targets_tr = [targets_tr[:,i] for i in range(targets_tr.size(1))]\n",
    "        targets_te = [targets_te[:,i] for i in range(targets_te.size(1))]\n",
    "        T_pred_tr = pred_tr[0].tolist()\n",
    "        A_pred_tr = pred_tr[1].tolist()\n",
    "        B_pred_tr = pred_tr[2].tolist()\n",
    "        C_pred_tr = pred_tr[3].tolist()\n",
    "\n",
    "        T_pred_te = pred_te[0].tolist()\n",
    "        A_pred_te = pred_te[1].tolist()\n",
    "        B_pred_te = pred_te[2].tolist()\n",
    "        C_pred_te = pred_te[3].tolist()\n",
    "\n",
    "        T_target_tr = targets_tr[0].tolist()\n",
    "        A_target_tr = targets_tr[1].tolist()\n",
    "        B_target_tr = targets_tr[2].tolist()\n",
    "        C_target_tr = targets_tr[3].tolist()\n",
    "\n",
    "        T_target_te = targets_te[0].tolist()\n",
    "        A_target_te = targets_te[1].tolist()\n",
    "        B_target_te = targets_te[2].tolist()\n",
    "        C_target_te = targets_te[3].tolist()\n",
    "\n",
    "        T_tr_r2 = pearsonr(T_pred_tr, T_target_tr)[0] ** 2\n",
    "        A_tr_r2 = pearsonr(A_pred_tr, A_target_tr)[0] ** 2\n",
    "        B_tr_r2 = pearsonr(B_pred_tr, B_target_tr)[0] ** 2\n",
    "        C_tr_r2 = pearsonr(C_pred_tr, C_target_tr)[0] ** 2\n",
    "\n",
    "        T_te_r2 = pearsonr(T_pred_te, T_target_te)[0] ** 2\n",
    "        A_te_r2 = pearsonr(A_pred_te, A_target_te)[0] ** 2\n",
    "        B_te_r2 = pearsonr(B_pred_te, B_target_te)[0] ** 2\n",
    "        C_te_r2 = pearsonr(C_pred_te, C_target_te)[0] ** 2\n",
    "\n",
    "        print(\"T Train R2: \", T_tr_r2)\n",
    "        print(\"A Train R2: \", A_tr_r2)\n",
    "        print(\"B Train R2: \", B_tr_r2)\n",
    "        print(\"C Train R2: \", C_tr_r2)\n",
    "\n",
    "        print(\"T Test R2: \", T_te_r2)\n",
    "        print(\"A Test R2: \", A_te_r2)\n",
    "        print(\"B Test R2: \", B_te_r2)\n",
    "        print(\"C Test R2: \", C_te_r2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf51e9a-8519-46a6-933c-bf7fc062a16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534286ca-2631-41ee-a45e-c75efec4dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# This function loads data from a specified folder, processes it, and splits it into training and testing sets. Returns the processed data as PyTorch tensors.\n",
    "# Keeping random_state fixed to ensure reproducibility as long as the data is not changed.\n",
    "\n",
    "def getData(feature_columns ,target_columns, folder):\n",
    "\n",
    "\n",
    "    all_items = os.listdir(folder)\n",
    "\n",
    "    files = [item for item in all_items if os.path.isfile(os.path.join(folder, item))]\n",
    "\n",
    "    dataFrame = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for i in files:\n",
    "        dataHelper = pd.read_csv(folder + i, sep='\\t', header=None, index_col=False)\n",
    "        info = i.split('_')\n",
    "        T = info[0]\n",
    "        A = info[1]\n",
    "        B = info[2]\n",
    "        C = info[3]\n",
    "        dataHelper = dataHelper.drop(index=[0])\n",
    "        dataHelper = dataHelper.drop(columns=[7])\n",
    "        dataHelper.columns = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']\n",
    "        dataHelper['T'] = T\n",
    "        dataHelper['A'] = A\n",
    "        dataHelper['B'] = B\n",
    "        C = C.removesuffix(\".txt\")\n",
    "        dataHelper['C'] = C\n",
    "        dataFrame = pd.concat([dataFrame, dataHelper], ignore_index=True)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataFrame[[feature_columns]],\n",
    "        dataFrame[target_columns],\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    x_train = torch.from_numpy(x_train.values).float()\n",
    "    x_test = torch.from_numpy(x_test.values).float()\n",
    "    y_train = torch.from_numpy(y_train.to_numpy(dtype=np.float32))\n",
    "    y_test = torch.from_numpy(y_test.to_numpy(dtype=np.float32))\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "# This function standardizes the data using StandardScaler, saves the scalers, and splits the data into training and testing sets. Returns the processed data as PyTorch tensors.\n",
    "# It also handles the conversion of certain values in the dataset based on specific conditions.\n",
    "# The function takes a scaler name, target columns, and an optional folder path as input parameters.\n",
    "#Keeping random_state fixed to ensure reproducibility as long as the data is not changed.\n",
    "\n",
    "def get_Standarized_data(scalerName,feature_columns ,target_columns,folder):\n",
    "\n",
    "    all_items = os.listdir(folder)\n",
    "\n",
    "    files = [item for item in all_items if os.path.isfile(os.path.join(folder, item))]\n",
    "\n",
    "    dataFrame = pd.DataFrame()\n",
    "\n",
    "    for i in files:\n",
    "        dataHelper = pd.read_csv(folder + i, sep='\\t', header=None, index_col=False)\n",
    "        info = i.split('_')\n",
    "        T = info[0]\n",
    "        A = info[1]\n",
    "        B = info[2]\n",
    "        C = info[3]\n",
    "        dataHelper = dataHelper.drop(index=[0])\n",
    "        dataHelper = dataHelper.drop(columns=[7])\n",
    "        dataHelper.columns = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']\n",
    "        dataHelper['T'] = T\n",
    "        dataHelper['A'] = A\n",
    "        dataHelper['B'] = B\n",
    "        C = C.removesuffix(\".txt\")\n",
    "\n",
    "        dataHelper['C'] = C\n",
    "        dataFrame = pd.concat([dataFrame, dataHelper], ignore_index=True)\n",
    "\n",
    "    #print(dataFrame.head())\n",
    "\n",
    "    features = dataFrame[feature_columns]\n",
    "    targets = dataFrame[target_columns]\n",
    "\n",
    "    featureScaler = StandardScaler()\n",
    "\n",
    "    standarized_features = featureScaler.fit_transform(features)\n",
    "\n",
    "    targetScaler = StandardScaler()\n",
    "\n",
    "    standarized_targets = targetScaler.fit_transform(targets)\n",
    "\n",
    "    joblib.dump(featureScaler, scalerName + '_featureScaler.pkl')\n",
    "    joblib.dump(targetScaler, scalerName + '_targetScaler.pkl')\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        standarized_features,\n",
    "        standarized_targets,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    x_test = torch.from_numpy(x_test).float()\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "    y_test = torch.from_numpy(y_test).float()\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n",
    "def get_data_chunks(feature_columns ,target_columns, folder):\n",
    "\n",
    "\n",
    "    all_items = os.listdir(folder)\n",
    "\n",
    "    files = [item for item in all_items if os.path.isfile(os.path.join(folder, item))]\n",
    "\n",
    "    dfList = []\n",
    "\n",
    "    for i in files:\n",
    "        dataHelper = pd.read_csv(folder + i, sep='\\t', header=None, index_col=False)\n",
    "        info = i.split('_')\n",
    "        T = info[0]\n",
    "        A = info[1]\n",
    "        B = info[2]\n",
    "        C = info[3]\n",
    "        dataHelper = dataHelper.drop(index=[0])\n",
    "        dataHelper = dataHelper.drop(columns=[7])\n",
    "        dataHelper.columns = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']\n",
    "        dataHelper['T'] = T\n",
    "        dataHelper['A'] = A\n",
    "        dataHelper['B'] = B\n",
    "        C = C.removesuffix(\".txt\")\n",
    "        dataHelper['C'] = C\n",
    "        features = dataHelper[feature_columns]\n",
    "        targets = dataHelper[target_columns]\n",
    "\n",
    "        features = torch.from_numpy(features.to_numpy(dtype=np.float32))\n",
    "        targets = torch.from_numpy(targets.to_numpy(dtype=np.float32))\n",
    "\n",
    "        dfList.append([features, targets])\n",
    "\n",
    "    return dfList\n",
    "\n",
    "\n",
    "def get_standarized_chunks(feature_scaler, target_scaler, feature_columns, target_columns, folder):\n",
    "\n",
    "    all_items = os.listdir(folder)\n",
    "    files = [item for item in all_items if os.path.isfile(os.path.join(folder, item))]\n",
    "\n",
    "    dfList = []\n",
    "\n",
    "    for i in files:\n",
    "        dataHelper = pd.read_csv(folder + i, sep='\\t', header=None, index_col=False)\n",
    "        info = i.split('_')\n",
    "        T = info[0]\n",
    "        A = info[1]\n",
    "        B = info[2]\n",
    "        C = info[3]\n",
    "        dataHelper = dataHelper.drop(index=[0])\n",
    "        dataHelper = dataHelper.drop(columns=[7])\n",
    "        dataHelper.columns = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']\n",
    "        dataHelper['T'] = T\n",
    "        dataHelper['A'] = A\n",
    "        dataHelper['B'] = B\n",
    "        C = C.removesuffix(\".txt\")\n",
    "        dataHelper['C'] = C\n",
    "        features = dataHelper[feature_columns]\n",
    "        targets = dataHelper[target_columns]\n",
    "\n",
    "        features_scaled = feature_scaler.transform(features)\n",
    "        targets_scaled = target_scaler.transform(targets)\n",
    "        features_scaled = np.array(features_scaled, dtype=np.float32)\n",
    "        targets_scaled = np.array(targets_scaled, dtype=np.float32)\n",
    "\n",
    "        features = torch.from_numpy(features_scaled)\n",
    "        targets = torch.from_numpy(targets_scaled)\n",
    "\n",
    "\n",
    "        dfList.append([features, targets])\n",
    "\n",
    "    return dfList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1325cc-5de1-437c-9761-3f5bff1f3bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

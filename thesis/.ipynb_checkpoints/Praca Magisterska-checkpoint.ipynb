{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a425640-4868-4a98-b4ff-acc9791ac398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, activation_fn=nn.ReLU):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        if len(hidden_layers) > 5:\n",
    "            raise ValueError(\"The number of hidden layers cannot exceed 5.\")\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Create hidden layers\n",
    "        for neurons in hidden_layers:\n",
    "            self.layers.append(nn.Linear(prev_size, neurons))\n",
    "            self.layers.append(activation_fn())  # Add the specified activation function\n",
    "            prev_size = neurons\n",
    "\n",
    "        # Create output layer\n",
    "        self.layers.append(nn.Linear(prev_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b4c141-f50a-424f-a382-cb11d495a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_model(model):\n",
    "    model = model.removesuffix(\".pth\")\n",
    "    layers = model.split('_')\n",
    "    layers.pop(0)\n",
    "    return list(map(int, layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f869b99-4685-4af5-9bca-2db9a25c6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def getData(target_columns=['T', 'A', 'B', 'C']):\n",
    "    project_folder = os.getcwd()\n",
    "    folder_path = os.path.join(project_folder,\"datasets\", \"new_Si_jaw_delta\", \"\")\n",
    "    #print(folder_path)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    all_items = os.listdir(folder_path)\n",
    "    #print(all_items)\n",
    "\n",
    "    # Filter the list to include only files\n",
    "    files = [item for item in all_items if os.path.isfile(os.path.join(folder_path, item))]\n",
    "\n",
    "    dataFrame = pd.DataFrame()\n",
    "\n",
    "    newDataFrame = pd.DataFrame()\n",
    "\n",
    "    for i in files:\n",
    "        dataHelper = pd.read_csv(folder_path + i, sep='\\t', header=None, index_col=False)\n",
    "        info = i.split('_')\n",
    "        T = info[0]\n",
    "        A = info[1]\n",
    "        B = info[2]\n",
    "        C = info[3]\n",
    "        dataHelper = dataHelper.drop(index=[0])\n",
    "        dataHelper = dataHelper.drop(columns=[7])\n",
    "        dataHelper.columns = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']\n",
    "        dataHelper['T'] = T\n",
    "        dataHelper['A'] = A\n",
    "        dataHelper['B'] = B\n",
    "        C = C.removesuffix(\".txt\")\n",
    "\n",
    "        if (\"-\" in C):\n",
    "            C = C.removesuffix(\"e-\")\n",
    "            C = float(C) * 10 ** -5\n",
    "        elif (\"e\" in C):\n",
    "            C = C.removesuffix(\"e\")\n",
    "            C = float(C) * 10 ** -5\n",
    "\n",
    "        if (float(C) > 1):\n",
    "            C = float(C) * 10 ** -5\n",
    "\n",
    "        dataHelper['C'] = C\n",
    "        dataFrame = pd.concat([dataFrame, dataHelper], ignore_index=True)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        dataFrame[['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']],\n",
    "        dataFrame[target_columns],\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    x_train = torch.from_numpy(x_train.values).float()\n",
    "    x_test = torch.from_numpy(x_test.values).float()\n",
    "    y_train = torch.from_numpy(y_train.to_numpy(dtype=np.float32))\n",
    "    y_test = torch.from_numpy(y_test.to_numpy(dtype=np.float32))\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f6b3cc0-71ea-42df-9a89-0ecddc071208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "\n",
    "def get_Standarized_data(scalerName ,target_columns=['T', 'A', 'B', 'C']):\n",
    "    project_folder = os.getcwd()\n",
    "    folder_path = os.path.join(project_folder,\"datasets\", \"new_Si_jaw_delta\", \"\")\n",
    "    #print(folder_path)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    all_items = os.listdir(folder_path)\n",
    "    #print(all_items)\n",
    "\n",
    "    # Filter the list to include only files\n",
    "    files = [item for item in all_items if os.path.isfile(os.path.join(folder_path, item))]\n",
    "\n",
    "    dataFrame = pd.DataFrame()\n",
    "\n",
    "    newDataFrame = pd.DataFrame()\n",
    "\n",
    "    for i in files:\n",
    "        dataHelper = pd.read_csv(folder_path + i, sep='\\t', header=None, index_col=False)\n",
    "        info = i.split('_')\n",
    "        T = info[0]\n",
    "        A = info[1]\n",
    "        B = info[2]\n",
    "        C = info[3]\n",
    "        dataHelper = dataHelper.drop(index=[0])\n",
    "        dataHelper = dataHelper.drop(columns=[7])\n",
    "        dataHelper.columns = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']\n",
    "        dataHelper['T'] = T\n",
    "        dataHelper['A'] = A\n",
    "        dataHelper['B'] = B\n",
    "        C = C.removesuffix(\".txt\")\n",
    "\n",
    "        if (\"-\" in C):\n",
    "            C = C.removesuffix(\"e-\")\n",
    "            C = float(C) * 10 ** -5\n",
    "        elif (\"e\" in C):\n",
    "            C = C.removesuffix(\"e\")\n",
    "            C = float(C) * 10 ** -5\n",
    "\n",
    "        if (float(C) > 1):\n",
    "            C = float(C) * 10 ** -5\n",
    "\n",
    "        dataHelper['C'] = C\n",
    "        dataFrame = pd.concat([dataFrame, dataHelper], ignore_index=True)\n",
    "\n",
    "    features = dataFrame[['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']]\n",
    "    targets = dataFrame[target_columns]\n",
    "\n",
    "    featureScaler = StandardScaler()\n",
    "\n",
    "    standarized_features = featureScaler.fit_transform(features)\n",
    "\n",
    "    targetScaler = StandardScaler()\n",
    "\n",
    "    standarized_targets = targetScaler.fit_transform(targets)\n",
    "\n",
    "    joblib.dump(featureScaler, scalerName + '_featureScaler.pkl')\n",
    "    joblib.dump(targetScaler, scalerName + '_targetScaler.pkl')\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        standarized_features,\n",
    "        standarized_targets,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    x_train = torch.from_numpy(x_train).float()\n",
    "    x_test = torch.from_numpy(x_test).float()\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "    y_test = torch.from_numpy(y_test).float()\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "166fff38-7a58-43ad-b46e-77399906875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 T:  0.9871178089912719\n",
      "R2 T Train:  0.9862036869551525\n",
      "R2 A:  0.8812675598619408\n",
      "R2 A Train:  0.8811413479267765\n",
      "R2 B:  0.0003384540400888341\n",
      "R2 B Train:  0.00012737624123682694\n",
      "R2 C:  2.337875362880518e-05\n",
      "R2 C Train:  6.900487528377103e-05\n",
      "R2 B Standard:  0.10282478062962934\n",
      "R2 B Standard Train:  0.1010459481627705\n",
      "R2 C Standard:  0.06281516224891774\n",
      "R2 C Standard Train:  0.06007684180897086\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "modelTT = \"modelT_48_32_16_8.pth\"\n",
    "modelAA = \"modelA_48_32_16_8.pth\"\n",
    "modelBB = \"modelB_256_256_256_128_64.pth\"\n",
    "modelCC = \"modelC_64_32_32_16.pth\"\n",
    "modelBBstandard = \"modelBstandard_48_32_32_16.pth\"\n",
    "modelCCstandard = \"modelCstandard_48_32_32_16.pth\"\n",
    "modelT = MLP(input_size=7, output_size=1, hidden_layers=decode_model(modelTT))\n",
    "modelA = MLP(input_size=7, output_size=1, hidden_layers=decode_model(modelAA))\n",
    "modelB = MLP(input_size=7, output_size=1, hidden_layers=decode_model(modelBB))\n",
    "modelC = MLP(input_size=7, output_size=1, hidden_layers=decode_model(modelCC))\n",
    "\n",
    "\n",
    "modelBstandard = MLP(input_size=7, output_size=1, hidden_layers=decode_model(modelBBstandard))\n",
    "modelCstandard = MLP(input_size=7, output_size=1, hidden_layers=decode_model(modelCCstandard))\n",
    "\n",
    "bScaler = joblib.load(\"bScaler_featureScaler.pkl\")\n",
    "bTargetScaler = joblib.load(\"bScaler_targetScaler.pkl\")\n",
    "cTargetScaler = joblib.load(\"cScaler_targetScaler.pkl\")\n",
    "cScaler = joblib.load(\"cScaler_featureScaler.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    modelT.load_state_dict(torch.load(modelTT))\n",
    "else:\n",
    "    modelT.load_state_dict(torch.load(modelTT, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    modelA.load_state_dict(torch.load(modelAA))\n",
    "else:\n",
    "    modelA.load_state_dict(torch.load(modelAA, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    modelC.load_state_dict(torch.load(modelCC))\n",
    "    modelB.load_state_dict(torch.load(modelBB))\n",
    "    modelBstandard.load_state_dict(torch.load(modelBBstandard))\n",
    "    modelCstandard.load_state_dict(torch.load(modelCCstandard))\n",
    "else:\n",
    "    modelC.load_state_dict(torch.load(modelCC,  map_location=torch.device('cpu')))\n",
    "    modelB.load_state_dict(torch.load(modelBB, map_location=torch.device('cpu')))\n",
    "    modelBstandard.load_state_dict(torch.load(modelBBstandard, map_location=torch.device('cpu')))\n",
    "    modelCstandard.load_state_dict(torch.load(modelCCstandard, map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "Tdata = getData(['T'])\n",
    "Adata = getData(['A'])\n",
    "Bdata = getData(['B'])\n",
    "Cdata = getData(['C'])\n",
    "BdataStandard = get_Standarized_data('bScaler', ['B'])\n",
    "CdataStandard = get_Standarized_data('cScaler', ['C'])\n",
    "\n",
    "tValues  = []\n",
    "tTrain = []\n",
    "aValues = []\n",
    "aTrain = []\n",
    "bValues = []\n",
    "bTrain = []\n",
    "cValues = []\n",
    "cTrain = []\n",
    "cStandardValues = []\n",
    "csTrain = []\n",
    "bStandardValues = []\n",
    "bsTrain = []\n",
    "\n",
    "\n",
    "for i in Tdata[2]:\n",
    "    with torch.no_grad():\n",
    "        tValues.append(modelT(i).item())\n",
    "\n",
    "for i in Tdata[0]:\n",
    "    with torch.no_grad():\n",
    "        tTrain.append(modelT(i).item())\n",
    "\n",
    "\n",
    "for i in Adata[2]:\n",
    "    with torch.no_grad():\n",
    "        aValues.append(modelA(i).item())\n",
    "\n",
    "for i in Adata[0]:\n",
    "    with torch.no_grad():\n",
    "        aTrain.append(modelA(i).item())\n",
    "\n",
    "for i in Bdata[2]:\n",
    "    with torch.no_grad():\n",
    "        bValues.append(modelB(i).item())\n",
    "\n",
    "for i in Bdata[0]:\n",
    "    with torch.no_grad():\n",
    "        bTrain.append(modelB(i).item())\n",
    "\n",
    "for i in Cdata[2]:\n",
    "    with torch.no_grad():\n",
    "        cValues.append(modelC(i).item())\n",
    "\n",
    "for i in Cdata[0]:\n",
    "    with torch.no_grad():\n",
    "        cTrain.append(modelC(i).item())\n",
    "\n",
    "\n",
    "featureNames = ['wavelength', 'psi65', 'del65', 'psi70', 'del70', 'psi75', 'del75']\n",
    "\n",
    "\n",
    "for i in BdataStandard[2]:\n",
    "    #a_df = pd.DataFrame(i.numpy().reshape(1, -1), columns=featureNames)\n",
    "    #j = bScaler.transform(a_df)\n",
    "    with torch.no_grad():\n",
    "        bStandardPred = modelBstandard(i)\n",
    "        #numpyBStandardPred = bStandardPred.detach().cpu().numpy()\n",
    "        #bInv = bTargetScaler.inverse_transform(numpyBStandardPred)\n",
    "        bStandardValues.append(bStandardPred.item())\n",
    "\n",
    "for i in BdataStandard[0]:\n",
    "    #a_df = pd.DataFrame(i.numpy().reshape(1, -1), columns=featureNames)\n",
    "    #j = bScaler.transform(a_df)\n",
    "    with torch.no_grad():\n",
    "        bStandardPred = modelBstandard(i)\n",
    "        #numpyBStandardPred = bStandardPred.detach().cpu().numpy()\n",
    "        #bInv = bTargetScaler.inverse_transform(numpyBStandardPred)\n",
    "        bsTrain.append(bStandardPred.item())\n",
    "\n",
    "\n",
    "\n",
    "for i in CdataStandard[2]:\n",
    "    #a_df = pd.DataFrame(i.numpy().reshape(1, -1), columns=featureNames)\n",
    "    #j = cScaler.transform(a_df)\n",
    "    with torch.no_grad():\n",
    "        cStandardPred = modelCstandard(i)\n",
    "        #numpyCStandardPred = cStandardPred.detach().cpu().numpy()\n",
    "        #cInv = cTargetScaler.inverse_transform(numpyCStandardPred)\n",
    "        cStandardValues.append(cStandardPred.item())\n",
    "\n",
    "for i in CdataStandard[0]:\n",
    "    #a_df = pd.DataFrame(i.numpy().reshape(1, -1), columns=featureNames)\n",
    "    #j = cScaler.transform(a_df)\n",
    "    with torch.no_grad():\n",
    "        cStandardPred = modelCstandard(i)\n",
    "        #numpyCStandardPred = cStandardPred.detach().cpu().numpy()\n",
    "        #cInv = cTargetScaler.inverse_transform(numpyCStandardPred)\n",
    "        csTrain.append(cStandardPred.item())\n",
    "\n",
    "r2t = r2_score(Tdata[3].tolist(), tValues)\n",
    "r2tTrain = r2_score(Tdata[1].tolist(), tTrain)\n",
    "r2a = r2_score(Adata[3].tolist(), aValues)\n",
    "r2aTrain = r2_score(Adata[1].tolist(), aTrain)\n",
    "r2b = r2_score(Bdata[3].tolist(), bValues)\n",
    "r2bTrain = r2_score(Bdata[1].tolist(), bTrain)\n",
    "r2c = r2_score(Cdata[3].tolist(), cValues)\n",
    "r2cTrain = r2_score(Cdata[1].tolist(), cTrain)\n",
    "r2bs = r2_score(BdataStandard[3].tolist(), bStandardValues)\n",
    "r2bsTrain = r2_score(BdataStandard[1].tolist(), bsTrain)\n",
    "r2cs = r2_score(CdataStandard[3].tolist(), cStandardValues)\n",
    "r2csTrain = r2_score(CdataStandard[1].tolist(), csTrain)\n",
    "\n",
    "tlist = Tdata[3].tolist()\n",
    "tlist1d = [item for sublist in tlist for item in sublist]\n",
    "ttlist = Tdata[1].tolist()\n",
    "ttlist1d = [item for sublist in ttlist for item in sublist]\n",
    "\n",
    "alist = Adata[3].tolist()\n",
    "alist1d = [item for sublist in alist for item in sublist]\n",
    "atlist = Adata[1].tolist()\n",
    "atlist1d = [item for sublist in atlist for item in sublist]\n",
    "\n",
    "blist = Bdata[3].tolist()\n",
    "blist1d = [item for sublist in blist for item in sublist]\n",
    "btlist = Bdata[1].tolist()\n",
    "btlist1d = [item for sublist in btlist for item in sublist]\n",
    "\n",
    "clist = Cdata[3].tolist()\n",
    "clist1d = [item for sublist in clist for item in sublist]\n",
    "ctlist = Cdata[1].tolist()\n",
    "ctlist1d = [item for sublist in ctlist for item in sublist]\n",
    "\n",
    "\n",
    "blistStandard = BdataStandard[3].tolist()\n",
    "blistStandard1d = [item for sublist in blistStandard for item in sublist]\n",
    "btlistStandard = BdataStandard[1].tolist()\n",
    "btlistStandard1d = [item for sublist in btlistStandard for item in sublist]\n",
    "\n",
    "clistStandard = CdataStandard[3].tolist()\n",
    "clistStandard1d = [item for sublist in clistStandard for item in sublist]\n",
    "ctlistStandard = CdataStandard[1].tolist()\n",
    "ctlistStandard1d = [item for sublist in ctlistStandard for item in sublist]\n",
    "\n",
    "\n",
    "\n",
    "pt = pearsonr(tlist1d, tValues)\n",
    "ptt = pearsonr(ttlist1d, tTrain)\n",
    "\n",
    "pa = pearsonr(alist1d, aValues)\n",
    "pat = pearsonr(atlist1d, aTrain)\n",
    "\n",
    "pb = pearsonr(blist1d, bValues)\n",
    "pbt = pearsonr(btlist1d, bTrain)\n",
    "\n",
    "pc = pearsonr(clist1d, cValues)\n",
    "pct = pearsonr(ctlist1d, cTrain)\n",
    "\n",
    "pbs = pearsonr(blistStandard1d , bStandardValues)\n",
    "pbst = pearsonr(btlistStandard1d , bsTrain)\n",
    "\n",
    "pcs = pearsonr(clistStandard1d, cStandardValues)\n",
    "pcst = pearsonr(ctlistStandard1d, csTrain)\n",
    "\n",
    "print(\"R2 T: \", pt[0]**2)\n",
    "print(\"R2 T Train: \", ptt[0]**2)\n",
    "print(\"R2 A: \", pa[0]**2)\n",
    "print(\"R2 A Train: \", pat[0]**2)\n",
    "print(\"R2 B: \", pb[0]**2)\n",
    "print(\"R2 B Train: \", pbt[0]**2)\n",
    "print(\"R2 C: \", pc[0]**2)\n",
    "print(\"R2 C Train: \", pct[0]**2)\n",
    "print(\"R2 B Standard: \", pbs[0]**2)\n",
    "print(\"R2 B Standard Train: \", pbst[0]**2)\n",
    "print(\"R2 C Standard: \", pcs[0]**2)\n",
    "print(\"R2 C Standard Train: \", pcst[0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65179489-454e-4dfd-80dd-974dbdd97b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
